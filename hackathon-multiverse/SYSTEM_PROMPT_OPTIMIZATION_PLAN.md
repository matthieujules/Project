# System Prompt Optimization Implementation Plan

## Overview
Transform the current conversation-turn optimization system into a system prompt optimization framework. Instead of optimizing individual messages, we'll optimize the instructions given to the mutator agent to find the most effective "sales bot personality" for achieving peace negotiations.

## Architecture Transformation

### Current → New Mapping
- **Node.prompt** → **Node.system_prompt** (instructions for mutator)
- **Node.reply** → **Node.conversation_samples** (test conversations generated)
- **Single conversation scoring** → **Multi-conversation average scoring**
- **Message mutator** → **System prompt mutator**
- **Turn-based exploration** → **Instruction-based exploration**

## Phase 1: Core Infrastructure Refactor

### 1.1 Schema Changes
**File: `backend/core/schemas.py`**
- Modify `Node` class:
  - `prompt: str` → `system_prompt: str` 
  - `reply: str` → `conversation_samples: List[dict]`
  - Add `avg_score: float` (average across conversation samples)
  - Add `sample_count: int` (number of test conversations)
  - Keep `score`, `depth`, `parent`, `emb`, `xy` (same meaning, different context)

### 1.2 System Prompt Mutator (New File)
**File: `backend/agents/system_prompt_mutator.py`**
- Replace `backend/agents/mutator.py` functionality
- Input: Parent system prompt + performance data
- Output: 3 variations of mutator instructions
- Examples: "focus on economic benefits", "acknowledge security concerns first", "use historical precedents"

### 1.3 Conversation Generator (New File)  
**File: `backend/core/conversation_generator.py`**
- Generate test conversations using a given system prompt
- Implement plateau-based dynamic stopping (Option 1)
- Use standard opening scenarios for fair comparison
- Min 3 turns, max 12 turns, stop when score plateaus

### 1.4 Modified Message Mutator
**File: `backend/agents/mutator.py` (MODIFY)**
- Accept `system_prompt` parameter instead of hardcoded instructions
- Use dynamic system prompt in messages[0]["content"]
- Keep same output format (message variants)

### 1.5 Node Storage Updates
**File: `backend/db/node_store.py` (MODIFY)**
- Update save/get functions to handle new Node schema
- Store `system_prompt` and `conversation_samples` instead of `prompt`/`reply`

### 1.6 Worker Process Refactor
**File: `backend/worker/parallel_worker.py` (MAJOR REFACTOR)**
- `process_node()`: 
  - Get parent system prompt
  - Generate 3 system prompt variants 
  - For each variant: generate multiple test conversations
  - Calculate average score across conversations
  - Save nodes with system prompts
- Keep same batch processing logic
- Keep same priority calculation

## Phase 2: Advanced Features & Integration

### 2.1 Enhanced Evaluation Framework
**File: `backend/core/evaluation.py` (NEW)**
- Robust system prompt testing across multiple scenarios
- Implement plateau detection stopping criteria:
  ```python
  # Stop if improvement < 5% per turn for last 3 turns
  # OR if score > 0.85 (early success)
  ```
- Calculate efficiency bonuses for shorter successful conversations

### 2.2 API Endpoint Updates
**File: `backend/api/routes.py` (MODIFY)**
- `/graph` endpoint: return system prompts instead of conversation turns
- `/conversation/{node_id}`: return sample conversations generated by that system prompt
- `/seed`: seed with initial system prompt instead of conversation prompt
- Keep same WebSocket broadcast logic

### 2.3 Visualization Updates
**File: `frontend/static/` (MODIFY)**
- Display system prompt text instead of conversation messages
- Show "Sample Conversations" panel for each node
- Color nodes by average score across conversation samples
- Update tooltips to show system prompt effectiveness

### 2.4 Database Migration
**File: `backend/db/migration.py` (NEW)**
- Convert existing conversation nodes to system prompt nodes
- Extract system prompts from conversation patterns
- Preserve priority queue structure

## Key Implementation Details

### Dynamic Stopping Logic
```python
def should_stop_conversation(scores: List[float], min_turns: int = 3) -> tuple[bool, float]:
    if len(scores) < min_turns:
        return False, 0.0
    
    # Plateau detection: < 5% improvement over last 3 turns
    recent_improvements = [scores[i] - scores[i-1] for i in range(-2, 0)]
    avg_improvement = sum(recent_improvements) / len(recent_improvements)
    
    if avg_improvement < 0.05 or scores[-1] > 0.85:
        return True, max(scores[-3:])
    
    return False, 0.0
```

### Test Conversation Generation
```python
async def generate_test_conversations(system_prompt: str) -> List[tuple[List[dict], float]]:
    scenarios = [
        {"opening": "President Putin, I believe we can find common ground...", "mood": "skeptical"},
        {"opening": "The economic costs are hurting both peoples...", "mood": "pragmatic"},
        {"opening": "I understand Russia's security concerns...", "mood": "defensive"}
    ]
    
    results = []
    for scenario in scenarios:
        conversation, score = await generate_single_conversation(system_prompt, scenario)
        results.append((conversation, score))
    
    return results
```

### Priority Calculation (UNCHANGED)
```python
# Same formula, different meaning:
# S = average score of system prompt across test conversations
# ΔS = improvement over parent system prompt's performance  
# similarity = semantic similarity between system prompt texts
# depth = generations of system prompt evolution
Priority = S + λ_trend*ΔS - λ_sim*similarity - λ_depth*depth
```

## Files to Modify vs Create

### Modify Existing Files
- `backend/core/schemas.py` - Update Node schema
- `backend/agents/mutator.py` - Accept dynamic system prompt
- `backend/agents/persona.py` - Keep unchanged (Putin stays same)
- `backend/agents/critic.py` - Keep unchanged (scoring logic same)
- `backend/worker/parallel_worker.py` - Major refactor for system prompt testing
- `backend/db/node_store.py` - Handle new schema
- `backend/api/routes.py` - Update endpoints
- `frontend/static/app.js` - Update visualization

### Create New Files
- `backend/agents/system_prompt_mutator.py` - Generate system prompt variants
- `backend/core/conversation_generator.py` - Test conversation generation
- `backend/core/evaluation.py` - Advanced evaluation framework
- `backend/db/migration.py` - Schema migration

## Success Metrics
- System discovers optimal system prompts that consistently achieve >0.8 scores
- Conversations naturally terminate based on plateau detection
- System prompt diversity maintained through similarity constraints
- Evolutionary discovery of effective persuasion strategies (empathy vs logic vs authority)

## Timeline
- **Phase 1**: 2-3 days (core refactor, basic functionality)
- **Phase 2**: 1-2 days (advanced features, visualization updates)

This transformation maintains the proven MCTS exploration framework while shifting the optimization target from "what to say" to "how to think about what to say" - enabling discovery of optimal agent instruction strategies.