from fastapi import APIRouter, HTTPException, Body
from typing import List
from backend.core.schemas import FocusZone, SettingsUpdate, Node
from backend.orchestrator.scheduler import boost_or_seed
from backend.config.settings import settings
from backend.core.logger import get_logger
from backend.db.redis_client import get_redis
from backend.db.node_store import get, save
from backend.db.frontier import push
from backend.core.utils import uuid_str
from backend.core.embeddings import embed, to_xy, fit_reducer
from backend.agents.system_prompt_mutator import generate_initial_system_prompts
from backend.core.evaluation import comprehensive_system_prompt_evaluation, compare_system_prompts, analyze_system_prompt_evolution

logger = get_logger(__name__)
router = APIRouter()


@router.post("/focus_zone")
async def focus_zone(payload: FocusZone):
    """
    Handle focus zone requests - either boost existing nodes or seed new ones.
    """
    try:
        result = await boost_or_seed(payload)
        return result
    except Exception as e:
        logger.error(f"Error processing focus zone: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.patch("/settings")
async def update_settings(updates: SettingsUpdate):
    """
    Update lambda values at runtime.
    """
    try:
        # Update only provided values
        if updates.lambda_trend is not None:
            settings.lambda_trend = updates.lambda_trend
        if updates.lambda_sim is not None:
            settings.lambda_sim = updates.lambda_sim
        if updates.lambda_depth is not None:
            settings.lambda_depth = updates.lambda_depth

        # Return full settings
        return {
            "lambda_trend": settings.lambda_trend,
            "lambda_sim": settings.lambda_sim,
            "lambda_depth": settings.lambda_depth,
            "redis_url": settings.redis_url,
            "log_level": settings.log_level,
        }
    except Exception as e:
        logger.error(f"Error updating settings: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/settings")
async def get_settings():
    """
    Get current settings.
    """
    return {
        "lambda_trend": settings.lambda_trend,
        "lambda_sim": settings.lambda_sim,
        "lambda_depth": settings.lambda_depth,
        "redis_url": settings.redis_url,
        "log_level": settings.log_level,
    }


@router.get("/graph")
async def get_graph():
    """
    Dump all system prompt nodes (id, xy, score, parent, system_prompt preview) â€“ UI calls once on load.
    """
    r = get_redis()
    nodes = []
    for key in r.keys("node:*"):
        node_id = key.decode('utf-8').replace("node:", "") if isinstance(key, bytes) else key.replace("node:", "")
        node = get(node_id)
        if node:
            # Include system prompt preview for visualization
            system_prompt_preview = node.system_prompt[:100] + "..." if len(node.system_prompt) > 100 else node.system_prompt
            
            nodes.append(
                {
                    "id": node.id,
                    "xy": node.xy,
                    "score": node.score,
                    "avg_score": getattr(node, 'avg_score', node.score),
                    "sample_count": getattr(node, 'sample_count', 0),
                    "parent": node.parent,
                    "depth": node.depth,
                    "system_prompt_preview": system_prompt_preview,
                }
            )
    return nodes


@router.get("/system_prompt/{node_id}")
async def get_system_prompt_details(node_id: str):
    """
    Get detailed information about a system prompt node.
    Returns the system prompt, conversation samples, and evaluation metrics.
    """
    try:
        # Get the target node
        target_node = get(node_id)
        
        if not target_node:
            return {"error": "System prompt node not found"}
        
        return {
            "node_id": node_id,
            "system_prompt": target_node.system_prompt,
            "depth": target_node.depth,
            "score": target_node.score,
            "avg_score": getattr(target_node, 'avg_score', target_node.score),
            "sample_count": getattr(target_node, 'sample_count', 0),
            "conversation_samples": getattr(target_node, 'conversation_samples', []),
            "parent": target_node.parent,
            "generation": target_node.depth
        }
    except Exception as e:
        return {"error": f"Failed to get system prompt details: {str(e)}"}


@router.get("/conversation_samples/{node_id}")
async def get_conversation_samples(node_id: str):
    """
    Get conversation samples generated by a specific system prompt.
    Returns the test conversations used to evaluate this system prompt.
    """
    try:
        # Get the target node
        target_node = get(node_id)
        
        if not target_node:
            return {"error": "System prompt node not found"}
        
        conversation_samples = getattr(target_node, 'conversation_samples', [])
        
        return {
            "node_id": node_id,
            "system_prompt": target_node.system_prompt[:200] + "..." if len(target_node.system_prompt) > 200 else target_node.system_prompt,
            "sample_count": len(conversation_samples),
            "avg_score": getattr(target_node, 'avg_score', target_node.score),
            "conversation_samples": conversation_samples
        }
    except Exception as e:
        return {"error": f"Failed to get conversation samples: {str(e)}"}


@router.post("/seed")
async def seed(system_prompt: str = Body(..., embed=True)):
    """
    Seed the system with an initial system prompt for optimization.
    If no system prompt provided, uses a default diplomatic approach.
    """
    try:
        # Use provided system prompt or generate a default one
        if not system_prompt or system_prompt.strip() == "":
            initial_prompts = await generate_initial_system_prompts(k=1)
            system_prompt = initial_prompts[0] if initial_prompts else (
                "You are a skilled diplomatic negotiator focused on finding common ground "
                "and building trust through respectful dialogue. Your goal is to guide "
                "conversations toward productive outcomes."
            )
        
        # Create system prompt node
        node = Node(
            id=uuid_str(),
            system_prompt=system_prompt,
            conversation_samples=[],
            score=0.5,  # Initial neutral score
            avg_score=0.5,
            sample_count=0,
            depth=0,
            emb=embed(system_prompt),
            xy=list(to_xy(embed(system_prompt))),
        )
        
        save(node)
        push(node.id, 1.0)  # High priority for initial exploration
        
        logger.info(f"Seeded system prompt optimization with node {node.id[:8]}...")
        
        return {
            "seed_id": node.id,
            "system_prompt": system_prompt[:100] + "..." if len(system_prompt) > 100 else system_prompt,
            "message": "System prompt optimization seeded successfully"
        }
        
    except Exception as e:
        logger.error(f"Error seeding system prompt: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/seed_multiple")
async def seed_multiple(num_seeds: int = Body(3, embed=True)):
    """
    Seed the system with multiple diverse initial system prompts for broader exploration.
    """
    try:
        # Generate diverse initial system prompts
        initial_prompts = await generate_initial_system_prompts(k=num_seeds)
        
        seed_ids = []
        for i, system_prompt in enumerate(initial_prompts):
            node = Node(
                id=uuid_str(),
                system_prompt=system_prompt,
                conversation_samples=[],
                score=0.5,
                avg_score=0.5,
                sample_count=0,
                depth=0,
                emb=embed(system_prompt),
                xy=list(to_xy(embed(system_prompt))),
            )
            
            save(node)
            push(node.id, 1.0 - (i * 0.1))  # Slightly different priorities
            seed_ids.append(node.id)
        
        logger.info(f"Seeded {len(seed_ids)} diverse system prompts for optimization")
        
        return {
            "seed_ids": seed_ids,
            "num_seeds": len(initial_prompts),
            "message": f"Seeded {len(initial_prompts)} diverse system prompts successfully"
        }
        
    except Exception as e:
        logger.error(f"Error seeding multiple system prompts: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/evaluate_system_prompt")
async def evaluate_system_prompt_endpoint(system_prompt: str = Body(..., embed=True)):
    """
    Evaluate a custom system prompt without adding it to the exploration tree.
    Useful for testing specific system prompt ideas.
    """
    try:
        if not system_prompt or system_prompt.strip() == "":
            raise HTTPException(status_code=400, detail="System prompt cannot be empty")
        
        # Perform comprehensive evaluation
        evaluation_results = await comprehensive_system_prompt_evaluation(system_prompt, num_tests=3)
        
        return {
            "system_prompt": system_prompt[:200] + "..." if len(system_prompt) > 200 else system_prompt,
            "evaluation": evaluation_results,
            "message": "System prompt evaluated successfully"
        }
        
    except Exception as e:
        logger.error(f"Error evaluating system prompt: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/compare_system_prompts")
async def compare_system_prompts_endpoint(system_prompts: List[str] = Body(...)):
    """
    Compare multiple system prompts head-to-head.
    Returns rankings and detailed comparison metrics.
    """
    try:
        if not system_prompts or len(system_prompts) < 2:
            raise HTTPException(status_code=400, detail="At least 2 system prompts required for comparison")
        
        if len(system_prompts) > 5:
            raise HTTPException(status_code=400, detail="Maximum 5 system prompts allowed for comparison")
        
        # Perform comparison
        comparison_results = await compare_system_prompts(system_prompts, num_tests=2)
        
        return {
            "num_prompts": len(system_prompts),
            "comparison": comparison_results,
            "message": f"Compared {len(system_prompts)} system prompts successfully"
        }
        
    except Exception as e:
        logger.error(f"Error comparing system prompts: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/evolution_analysis")
async def get_evolution_analysis():
    """
    Analyze the evolution of system prompts in the current exploration.
    Returns insights about improvement trends and best performing approaches.
    """
    try:
        analysis_results = await analyze_system_prompt_evolution()
        
        return {
            "analysis": analysis_results,
            "message": "Evolution analysis completed successfully"
        }
        
    except Exception as e:
        logger.error(f"Error analyzing evolution: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/best_system_prompts")
async def get_best_system_prompts(limit: int = 10):
    """
    Get the best performing system prompts from the current exploration.
    """
    try:
        r = get_redis()
        all_nodes = []
        
        # Get all nodes and their scores
        for key in r.keys("node:*"):
            node_id = key.decode('utf-8').replace("node:", "") if isinstance(key, bytes) else key.replace("node:", "")
            node = get(node_id)
            if node and hasattr(node, 'score') and node.score is not None:
                all_nodes.append(node)
        
        # Sort by score (descending) and take top performers
        best_nodes = sorted(all_nodes, key=lambda x: x.score or 0, reverse=True)[:limit]
        
        best_prompts = []
        for node in best_nodes:
            best_prompts.append({
                "node_id": node.id,
                "system_prompt": node.system_prompt,
                "score": node.score,
                "avg_score": getattr(node, 'avg_score', node.score),
                "sample_count": getattr(node, 'sample_count', 0),
                "generation": node.depth,
                "system_prompt_preview": node.system_prompt[:150] + "..." if len(node.system_prompt) > 150 else node.system_prompt
            })
        
        return {
            "best_system_prompts": best_prompts,
            "total_found": len(best_nodes),
            "message": f"Retrieved top {len(best_nodes)} system prompts"
        }
        
    except Exception as e:
        logger.error(f"Error getting best system prompts: {e}")
        raise HTTPException(status_code=500, detail=str(e))
